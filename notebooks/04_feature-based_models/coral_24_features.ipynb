{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPLQ90KXv0-p"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, cohen_kappa_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDxKc1o1v98l"
      },
      "outputs": [],
      "source": [
        "class FeatureCORAL(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes=8):\n",
        "        super(FeatureCORAL, self).__init__()\n",
        "        # 1. Shared Weight Vector (w): Projects features to a single \"Proficiency Score\"\n",
        "        self.linear = nn.Linear(input_dim, 1, bias=False)\n",
        "\n",
        "        # 2. Independent Biases (b_k): The cut-points between levels\n",
        "        # We need K-1 biases for K classes\n",
        "        self.biases = nn.Parameter(torch.zeros(num_classes - 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, input_dim]\n",
        "        # score shape: [batch_size, 1]\n",
        "        score = self.linear(x)\n",
        "\n",
        "        # Adding biases: [batch_size, 1] + [K-1] -> [batch_size, K-1] (Broadcasting)\n",
        "        logits = score + self.biases\n",
        "        return logits\n",
        "\n",
        "def task_encoding(y, num_classes=8):\n",
        "    batch_size = y.size(0)\n",
        "    # Create a matrix of thresholds [0, 1, ..., K-2]\n",
        "    thresholds = torch.arange(num_classes - 1).expand(batch_size, num_classes - 1).to(y.device)\n",
        "    # Broadcast y: [batch_size, 1]\n",
        "    y_expanded = y.view(-1, 1).expand(batch_size, num_classes - 1)\n",
        "\n",
        "    # Return 1 if label > threshold\n",
        "    return (y_expanded > thresholds).float()\n",
        "\n",
        "def predict_rank(logits):\n",
        "    \"\"\"Sum probabilities to get expected rank (Proba -> Sigmoid -> Sum -> Round)\"\"\"\n",
        "    probs = torch.sigmoid(logits)\n",
        "    pred_rank = torch.sum(probs, dim=1)\n",
        "    return torch.round(pred_rank).detach().cpu().numpy().astype(int)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h5X41EYwBc3"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv(\"training_features_extended_final.csv\")  # Make sure this filename is correct\n",
        "target_col = 'label' if 'label' in df.columns else 'labels'\n",
        "\n",
        "X = df.drop(columns=[target_col]).values.astype(np.float32)\n",
        "y = df[target_col].values.astype(np.int64)\n",
        "num_classes = 8\n",
        "input_dim = X.shape[1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_0qRkppwH6W",
        "outputId": "e5a34942-2ee6-4afe-b17d-666af3f506f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Feature-CORAL (PyTorch) on 24 features...\n",
            "------------------------------------------------------------\n",
            "Fold 1: QWK = 0.7593 | MAE = 0.6184 | RMSE = 0.9711\n",
            "Fold 2: QWK = 0.7419 | MAE = 0.7105 | RMSE = 1.0044\n",
            "Fold 3: QWK = 0.7562 | MAE = 0.6623 | RMSE = 0.9665\n",
            "Fold 4: QWK = 0.7676 | MAE = 0.6564 | RMSE = 0.9457\n",
            "Fold 5: QWK = 0.7703 | MAE = 0.6916 | RMSE = 0.9549\n",
            "------------------------------------------------------------\n",
            "FINAL RESULTS (Feature-CORAL):\n",
            "MAE: 0.6678 ± 0.0316\n",
            "RMSE: 0.9685 ± 0.0200\n",
            "QWK: 0.7591 ± 0.0100\n"
          ]
        }
      ],
      "source": [
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "results = {'MAE': [], 'RMSE': [], 'QWK': []}\n",
        "fold = 1\n",
        "\n",
        "print(f\"Starting Feature-CORAL (PyTorch) on {input_dim} features...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    X_train_t = torch.tensor(X_train)\n",
        "    y_train_t = torch.tensor(y_train)\n",
        "    X_test_t = torch.tensor(X_test)\n",
        "\n",
        "    model = FeatureCORAL(input_dim, num_classes)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "    loss_fn = nn.BCEWithLogitsLoss() # Binary Cross Entropy\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    epochs = 1000  # Features are simple, converges fast\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(X_train_t)\n",
        "\n",
        "        # Create CORAL targets: [1,1,0,0...]\n",
        "        targets = task_encoding(y_train_t, num_classes)\n",
        "\n",
        "        loss = loss_fn(logits, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_logits = model(X_test_t)\n",
        "        y_pred = predict_rank(test_logits)\n",
        "\n",
        "\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    qwk = cohen_kappa_score(y_test, y_pred, weights='quadratic')\n",
        "\n",
        "    results['MAE'].append(mae)\n",
        "    results['RMSE'].append(rmse)\n",
        "    results['QWK'].append(qwk)\n",
        "\n",
        "    print(f\"Fold {fold}: QWK = {qwk:.4f} | MAE = {mae:.4f} | RMSE = {rmse:.4f}\")\n",
        "    fold += 1\n",
        "\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(\"FINAL RESULTS (Feature-CORAL):\")\n",
        "for metric, values in results.items():\n",
        "    print(f\"{metric}: {np.mean(values):.4f} ± {np.std(values):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
