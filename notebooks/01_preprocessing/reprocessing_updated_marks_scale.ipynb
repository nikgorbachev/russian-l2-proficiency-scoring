{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19f5d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_mappings = {\n",
    "    2020: {\n",
    "        \"file\": \"../../data/assessment_csv/2020.csv\",\n",
    "        \"last_name_col\": \"Last Name\",\n",
    "        \"first_name_col\": \"First Name\",\n",
    "        \"entrance_col\": \"Writing\",\n",
    "        \"exit_col\": \"Exit Writing\"\n",
    "    },\n",
    "    2019: {\n",
    "        \"file\": \"../../data/assessment_csv/2019.csv\",\n",
    "        \"last_name_col\": \"Last Name\",\n",
    "        \"first_name_col\": \"First Name\",\n",
    "        \"entrance_col\": \"Writing Entrance\",\n",
    "        \"exit_col\": \"Writing test Final \"\n",
    "    },\n",
    "    2018: {\n",
    "        \"file\": \"../../data/assessment_csv/2018.csv\",\n",
    "        \"last_name_col\": \"Last Name\",\n",
    "        \"first_name_col\": \"First Name\",\n",
    "        \"entrance_col\": \"Writing Entrance\",\n",
    "        \"exit_col\": \"Writing test Final \"\n",
    "    },\n",
    "    2017: {\n",
    "        \"file\": \"../../data/assessment_csv/2017.csv\",\n",
    "        \"last_name_col\": \"Last Name\",\n",
    "        \"first_name_col\": \"First Name\",\n",
    "        \"entrance_col\": \"Writing Entrance\",\n",
    "        \"exit_col\": \"Writing test Final \"\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7800b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_marks_dfs = []\n",
    "\n",
    "for year, mapping in file_mappings.items():\n",
    "    file_name = mapping[\"file\"]\n",
    "    \n",
    "    df = pd.read_csv(file_name, sep='\\t')\n",
    "    \n",
    "    col_mapping = {\n",
    "        mapping[\"last_name_col\"]: \"Last Name\",\n",
    "        mapping[\"first_name_col\"]: \"First Name\",\n",
    "        mapping[\"entrance_col\"]: \"Entrance\",\n",
    "        mapping[\"exit_col\"]: \"Exit\"\n",
    "    }\n",
    "    \n",
    "    df_selected = df[list(col_mapping.keys())].rename(columns=col_mapping)\n",
    "    \n",
    "    df_selected[\"Year\"] = year\n",
    "    \n",
    "    all_marks_dfs.append(df_selected)\n",
    "\n",
    "\n",
    "final_marks_df = pd.concat(all_marks_dfs, ignore_index=True)\n",
    "\n",
    "final_marks_df.to_csv(\"merged_raw_marks_step1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d55bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(final_marks_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3491568",
   "metadata": {},
   "source": [
    "```<class 'pandas.DataFrame'>\n",
    "RangeIndex: 493 entries, 0 to 492\n",
    "Data columns (total 5 columns):\n",
    " #   Column      Non-Null Count  Dtype\n",
    "---  ------      --------------  -----\n",
    " 0   Last Name   493 non-null    str  \n",
    " 1   First Name  493 non-null    str  \n",
    " 2   Entrance    386 non-null    str  \n",
    " 3   Exit        473 non-null    str  \n",
    " 4   Year        493 non-null    int64\n",
    "dtypes: int64(1), str(4)\n",
    "memory usage: 19.4 KB\n",
    "None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d321ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "df_marks = pd.read_csv(\"./preprocessed/merged_raw_marks_step1.csv\")\n",
    "\n",
    "\n",
    "# --- Anonymization  ---\n",
    "df_marks['Student_Key'] = df_marks['Last Name'].str.strip().str.upper() + '_' + df_marks['First Name'].str.strip().str.upper()\n",
    "\n",
    "# Getting the unique keys and assigning sequential Student_IDs\n",
    "unique_students = df_marks[['Student_Key']].drop_duplicates().reset_index(drop=True)\n",
    "unique_students['Student_ID'] = 'STUDENT_' + (unique_students.index + 1).astype(str).str.zfill(4)\n",
    "\n",
    "# Creating the final lookup table (Last Name, First Name, Student_ID)\n",
    "anonymization_lookup = df_marks[['Last Name', 'First Name', 'Student_Key']].drop_duplicates()\n",
    "anonymization_lookup = pd.merge(anonymization_lookup, unique_students, on='Student_Key')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26cba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymization_lookup.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4938b17",
   "metadata": {},
   "source": [
    "```\n",
    "<class 'pandas.DataFrame'>\n",
    "RangeIndex: 472 entries, 0 to 471\n",
    "Data columns (total 4 columns):\n",
    " #   Column       Non-Null Count  Dtype\n",
    "---  ------       --------------  -----\n",
    " 0   Last Name    472 non-null    str  \n",
    " 1   First Name   472 non-null    str  \n",
    " 2   Student_Key  472 non-null    str  \n",
    " 3   Student_ID   472 non-null    str  \n",
    "dtypes: str(4)\n",
    "memory usage: 14.9 KB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a57552",
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymization_lookup.to_csv('anonymization_lookup_upd.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e5bee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings_from_mid_to_actfl = {\n",
    "\n",
    "'NL': 0.0, 'NL+': '0.0+',\n",
    "\n",
    "'NM-': '0.5-', 'NM': 0.5, 'NM+': '0.5+',\n",
    "\n",
    "'NH-': '0.9-', 'NH': 0.9, 'NH+': '0.9+',\n",
    "\n",
    "'IL-': '1.0-', 'IL': 1.0, 'IL+': '1.0+',\n",
    "\n",
    "'IM-': '1.5=', 'IM': 1.5, 'IM+': '1.5+',\n",
    "\n",
    "'IH-': '1.9+', 'IH': 1.9,'IH+': '1.9+',\n",
    "\n",
    "'AL-': '2.0-', 'AL': 2, 'AL+': '2.0+',\n",
    "\n",
    "'AM-': '2.5-', 'AM': 2.5, 'AM+': '2.5+',\n",
    "\n",
    "'AH-': '2.9+', 'AH': 2.9, 'AH+': '2.9+',\n",
    "\n",
    "'S': 3.0,\n",
    "\n",
    "'NO.TEST': np.nan, 'NOTEST': np.nan, 'NOSHOW': np.nan, 'NT': np.nan,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6709e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# --- Score Standardization to 27-Point Ordinal Scale (0-24 Index) ---\n",
    "\n",
    "# Master mapping dictionary: Raw Score/Label (Key) -> Ordinal Index (Value)\n",
    "# Note: The mapping below omits NL levels (0.0/0.0+) as they are the very lowest and are sparse.\n",
    "# The scale starts at NM- (Index 0). The total range is 0 to 24 (25 distinct values).\n",
    "\n",
    "# 1. Define the core ACTFL labels and their corresponding starting ordinal index\n",
    "actfl_sub_levels = [\n",
    "    ('NL', 0), ('NM', 1), ('NH', 2), \n",
    "    ('IL', 3), ('IM', 4), ('IH', 5), \n",
    "    ('AL', 6), ('AM', 7), ('AH', 8), \n",
    "    ('S', 9)\n",
    "]\n",
    "\n",
    "granular_map = {}\n",
    "\n",
    "# 2. Map ACTFL Labels (e.g., 'IM-', 'IM', 'IM+') to sequential indices\n",
    "for label, start_index in actfl_sub_levels:\n",
    "        # Map sub-levels: X- => index, X => index+1, X+ => index+2\n",
    "        granular_map[f'{label}-'] = start_index \n",
    "        granular_map[label] = start_index \n",
    "        granular_map[f'{label}+'] = start_index\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6af076b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NL-': nan,\n",
       " 'NL': nan,\n",
       " 'NL+': nan,\n",
       " 'NM-': 1,\n",
       " 'NM': 1,\n",
       " 'NM+': 1,\n",
       " 'NH-': 2,\n",
       " 'NH': 2,\n",
       " 'NH+': 2,\n",
       " 'IL-': 3,\n",
       " 'IL': 3,\n",
       " 'IL+': 3,\n",
       " 'IM-': 4,\n",
       " 'IM': 4,\n",
       " 'IM+': 4,\n",
       " 'IH-': 5,\n",
       " 'IH': 5,\n",
       " 'IH+': 5,\n",
       " 'AL-': 6,\n",
       " 'AL': 6,\n",
       " 'AL+': 6,\n",
       " 'AM-': 7,\n",
       " 'AM': 7,\n",
       " 'AM+': 7,\n",
       " 'AH-': 8,\n",
       " 'AH': 8,\n",
       " 'AH+': 8,\n",
       " 'S-': 9,\n",
       " 'S': 9,\n",
       " 'S+': 9,\n",
       " '0.0-': 0,\n",
       " '0.0': 0,\n",
       " '0.0+': 0,\n",
       " '0.5-': 1,\n",
       " '0.5': 1,\n",
       " '0.5+': 1,\n",
       " '0.9-': 2,\n",
       " '0.9': 2,\n",
       " '0.9+': 2,\n",
       " '1.0-': 3,\n",
       " '1.0': 3,\n",
       " '1.0+': 3,\n",
       " '1.5-': 4,\n",
       " '1.5': 4,\n",
       " '1.5+': 14,\n",
       " '1.9-': 5,\n",
       " '1.9': 5,\n",
       " '1.9+': 5,\n",
       " '2.0-': 6,\n",
       " '2.0': 6,\n",
       " '2.0+': 6,\n",
       " '2.5-': 7,\n",
       " '2.5': 7,\n",
       " '2.5+': 7,\n",
       " '2.9-': 8,\n",
       " '2.9': 8,\n",
       " '2.9+': 8,\n",
       " '3.0-': 9,\n",
       " '3.0': 9,\n",
       " '3.0+': 9,\n",
       " '0,0-': 0,\n",
       " '0,0': 0,\n",
       " '0,0+': 0,\n",
       " '0,5-': 1,\n",
       " '0,5': 1,\n",
       " '0,5+': 1,\n",
       " '0,9-': 2,\n",
       " '0,9': 2,\n",
       " '0,9+': 2,\n",
       " '1,0-': 3,\n",
       " '1,0': 3,\n",
       " '1,0+': 3,\n",
       " '1,5-': 4,\n",
       " '1,5': 4,\n",
       " '1,5+': 14,\n",
       " '1,9-': 5,\n",
       " '1,9': 5,\n",
       " '1,9+': 5,\n",
       " '2,0-': 6,\n",
       " '2,0': 6,\n",
       " '2,0+': 6,\n",
       " '2,5-': 7,\n",
       " '2,5': 7,\n",
       " '2,5+': 7,\n",
       " '2,9-': 8,\n",
       " '2,9': 8,\n",
       " '2,9+': 8,\n",
       " '3,0-': 9,\n",
       " '3,0': 9,\n",
       " '3,0+': 9,\n",
       " 'GROUP': nan,\n",
       " 'NO.TEST': nan,\n",
       " 'NOTEST': nan,\n",
       " 'NOSHOW': nan,\n",
       " 'NT': nan}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "granular_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8148f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Map Raw Numeric Strings to the same index\n",
    "# Note: We must handle comma/dot and the special '=' sign (from '1.5=')\n",
    "\n",
    "# ACTFL Base Numeric Points: 0.5, 0.9, 1.0, 1.5, 1.9, 2.0, 2.5, 2.9, 3.0\n",
    "base_numeric_map = {\n",
    "    '0.0-': 0, '0.0': 0, '0.0+': 0,\n",
    "    '0.5-': 1, '0.5': 1, '0.5+': 1, # NM base\n",
    "    '0.9-': 2, '0.9': 2, '0.9+': 2, # NH base\n",
    "    '1.0-': 3, '1.0': 3, '1.0+': 3, # IL base\n",
    "    '1.5-': 4, '1.5': 4, '1.5+': 4, # IM base\n",
    "    '1.9-': 5, '1.9': 5, '1.9+': 5, # IH base\n",
    "    '2.0-': 6, '2.0': 6, '2.0+': 6, # AL base\n",
    "    '2.5-': 7, '2.5': 7, '2.5+': 7, # AM base\n",
    "    '2.9-': 8, '2.9': 8, '2.9+': 8, # AH base\n",
    "    '3.0-': 9, '3.0': 9, '3.0+': 9\n",
    "}\n",
    "\n",
    "# Updating granular_map with these numeric-string mappings\n",
    "granular_map.update(base_numeric_map)\n",
    "\n",
    "# Adding numeric keys with comma decimals (e.g., \"1,5\" -> 10)\n",
    "granular_map_commas = {k.replace('.', ','): v for k, v in base_numeric_map.items() if '.' in k}\n",
    "granular_map.update(granular_map_commas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f04460b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NL-': nan,\n",
       " 'NL': nan,\n",
       " 'NL+': nan,\n",
       " 'NM-': 1,\n",
       " 'NM': 1,\n",
       " 'NM+': 1,\n",
       " 'NH-': 2,\n",
       " 'NH': 2,\n",
       " 'NH+': 2,\n",
       " 'IL-': 3,\n",
       " 'IL': 3,\n",
       " 'IL+': 3,\n",
       " 'IM-': 4,\n",
       " 'IM': 4,\n",
       " 'IM+': 4,\n",
       " 'IH-': 5,\n",
       " 'IH': 5,\n",
       " 'IH+': 5,\n",
       " 'AL-': 6,\n",
       " 'AL': 6,\n",
       " 'AL+': 6,\n",
       " 'AM-': 7,\n",
       " 'AM': 7,\n",
       " 'AM+': 7,\n",
       " 'AH-': 8,\n",
       " 'AH': 8,\n",
       " 'AH+': 8,\n",
       " 'S-': 9,\n",
       " 'S': 9,\n",
       " 'S+': 9,\n",
       " '0.0-': 0,\n",
       " '0.0': 0,\n",
       " '0.0+': 0,\n",
       " '0.5-': 1,\n",
       " '0.5': 1,\n",
       " '0.5+': 1,\n",
       " '0.9-': 2,\n",
       " '0.9': 2,\n",
       " '0.9+': 2,\n",
       " '1.0-': 3,\n",
       " '1.0': 3,\n",
       " '1.0+': 3,\n",
       " '1.5-': 4,\n",
       " '1.5': 4,\n",
       " '1.5+': 4,\n",
       " '1.9-': 5,\n",
       " '1.9': 5,\n",
       " '1.9+': 5,\n",
       " '2.0-': 6,\n",
       " '2.0': 6,\n",
       " '2.0+': 6,\n",
       " '2.5-': 7,\n",
       " '2.5': 7,\n",
       " '2.5+': 7,\n",
       " '2.9-': 8,\n",
       " '2.9': 8,\n",
       " '2.9+': 8,\n",
       " '3.0-': 9,\n",
       " '3.0': 9,\n",
       " '3.0+': 9,\n",
       " '0,0-': 0,\n",
       " '0,0': 0,\n",
       " '0,0+': 0,\n",
       " '0,5-': 1,\n",
       " '0,5': 1,\n",
       " '0,5+': 1,\n",
       " '0,9-': 2,\n",
       " '0,9': 2,\n",
       " '0,9+': 2,\n",
       " '1,0-': 3,\n",
       " '1,0': 3,\n",
       " '1,0+': 3,\n",
       " '1,5-': 4,\n",
       " '1,5': 4,\n",
       " '1,5+': 4,\n",
       " '1,9-': 5,\n",
       " '1,9': 5,\n",
       " '1,9+': 5,\n",
       " '2,0-': 6,\n",
       " '2,0': 6,\n",
       " '2,0+': 6,\n",
       " '2,5-': 7,\n",
       " '2,5': 7,\n",
       " '2,5+': 7,\n",
       " '2,9-': 8,\n",
       " '2,9': 8,\n",
       " '2,9+': 8,\n",
       " '3,0-': 9,\n",
       " '3,0': 9,\n",
       " '3,0+': 9,\n",
       " 'GROUP': nan,\n",
       " 'NO.TEST': nan,\n",
       " 'NOTEST': nan,\n",
       " 'NOSHOW': nan,\n",
       " 'NT': nan}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "granular_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3446375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "granular_map_df = pd.DataFrame(list(granular_map.items()), columns=['key', 'value'])\n",
    "granular_map_df.to_csv('granular_marks_upd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "43bca02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NL-': nan, 'NL': nan, 'NL+': nan, 'NM-': 1, 'NM': 1, 'NM+': 1, 'NH-': 2, 'NH': 2, 'NH+': 2, 'IL-': 3, 'IL': 3, 'IL+': 3, 'IM-': 4, 'IM': 4, 'IM+': 4, 'IH-': 5, 'IH': 5, 'IH+': 5, 'AL-': 6, 'AL': 6, 'AL+': 6, 'AM-': 7, 'AM': 7, 'AM+': 7, 'AH-': 8, 'AH': 8, 'AH+': 8, 'S-': 9, 'S': 9, 'S+': 9, '0.0-': 0, '0.0': 0, '0.0+': 0, '0.5-': 1, '0.5': 1, '0.5+': 1, '0.9-': 2, '0.9': 2, '0.9+': 2, '1.0-': 3, '1.0': 3, '1.0+': 3, '1.5-': 4, '1.5': 4, '1.5+': 4, '1.9-': 5, '1.9': 5, '1.9+': 5, '2.0-': 6, '2.0': 6, '2.0+': 6, '2.5-': 7, '2.5': 7, '2.5+': 7, '2.9-': 8, '2.9': 8, '2.9+': 8, '3.0-': 9, '3.0': 9, '3.0+': 9, '0,0-': 0, '0,0': 0, '0,0+': 0, '0,5-': 1, '0,5': 1, '0,5+': 1, '0,9-': 2, '0,9': 2, '0,9+': 2, '1,0-': 3, '1,0': 3, '1,0+': 3, '1,5-': 4, '1,5': 4, '1,5+': 4, '1,9-': 5, '1,9': 5, '1,9+': 5, '2,0-': 6, '2,0': 6, '2,0+': 6, '2,5-': 7, '2,5': 7, '2,5+': 7, '2,9-': 8, '2,9': 8, '2,9+': 8, '3,0-': 9, '3,0': 9, '3,0+': 9, 'GROUP': nan, 'NO.TEST': nan, 'NOTEST': nan, 'NOSHOW': nan, 'NT': nan}\n"
     ]
    }
   ],
   "source": [
    "print(granular_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e57b5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add other non-score labels\n",
    "granular_map.update({\n",
    "    'NL-': np.nan, 'NL': np.nan, 'NL+': np.nan, # Excluding NL from the scale (mapped to NaN)\n",
    "    'GROUP': np.nan, 'NO.TEST': np.nan, 'NOTEST': np.nan, 'NOSHOW': np.nan, 'NT': np.nan\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "af84d39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NL-': nan,\n",
       " 'NL': nan,\n",
       " 'NL+': nan,\n",
       " 'NM-': 1,\n",
       " 'NM': 1,\n",
       " 'NM+': 1,\n",
       " 'NH-': 2,\n",
       " 'NH': 2,\n",
       " 'NH+': 2,\n",
       " 'IL-': 3,\n",
       " 'IL': 3,\n",
       " 'IL+': 3,\n",
       " 'IM-': 4,\n",
       " 'IM': 4,\n",
       " 'IM+': 4,\n",
       " 'IH-': 5,\n",
       " 'IH': 5,\n",
       " 'IH+': 5,\n",
       " 'AL-': 6,\n",
       " 'AL': 6,\n",
       " 'AL+': 6,\n",
       " 'AM-': 7,\n",
       " 'AM': 7,\n",
       " 'AM+': 7,\n",
       " 'AH-': 8,\n",
       " 'AH': 8,\n",
       " 'AH+': 8,\n",
       " 'S-': 9,\n",
       " 'S': 9,\n",
       " 'S+': 9,\n",
       " '0.0-': 0,\n",
       " '0.0': 0,\n",
       " '0.0+': 0,\n",
       " '0.5-': 1,\n",
       " '0.5': 1,\n",
       " '0.5+': 1,\n",
       " '0.9-': 2,\n",
       " '0.9': 2,\n",
       " '0.9+': 2,\n",
       " '1.0-': 3,\n",
       " '1.0': 3,\n",
       " '1.0+': 3,\n",
       " '1.5-': 4,\n",
       " '1.5': 4,\n",
       " '1.5+': 4,\n",
       " '1.9-': 5,\n",
       " '1.9': 5,\n",
       " '1.9+': 5,\n",
       " '2.0-': 6,\n",
       " '2.0': 6,\n",
       " '2.0+': 6,\n",
       " '2.5-': 7,\n",
       " '2.5': 7,\n",
       " '2.5+': 7,\n",
       " '2.9-': 8,\n",
       " '2.9': 8,\n",
       " '2.9+': 8,\n",
       " '3.0-': 9,\n",
       " '3.0': 9,\n",
       " '3.0+': 9,\n",
       " '0,0-': 0,\n",
       " '0,0': 0,\n",
       " '0,0+': 0,\n",
       " '0,5-': 1,\n",
       " '0,5': 1,\n",
       " '0,5+': 1,\n",
       " '0,9-': 2,\n",
       " '0,9': 2,\n",
       " '0,9+': 2,\n",
       " '1,0-': 3,\n",
       " '1,0': 3,\n",
       " '1,0+': 3,\n",
       " '1,5-': 4,\n",
       " '1,5': 4,\n",
       " '1,5+': 4,\n",
       " '1,9-': 5,\n",
       " '1,9': 5,\n",
       " '1,9+': 5,\n",
       " '2,0-': 6,\n",
       " '2,0': 6,\n",
       " '2,0+': 6,\n",
       " '2,5-': 7,\n",
       " '2,5': 7,\n",
       " '2,5+': 7,\n",
       " '2,9-': 8,\n",
       " '2,9': 8,\n",
       " '2,9+': 8,\n",
       " '3,0-': 9,\n",
       " '3,0': 9,\n",
       " '3,0+': 9,\n",
       " 'GROUP': nan,\n",
       " 'NO.TEST': nan,\n",
       " 'NOTEST': nan,\n",
       " 'NOSHOW': nan,\n",
       " 'NT': nan}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "granular_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd07c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def standardize_to_ordinal(score):\n",
    "    \"\"\"Converts a raw score string/value to the unified ordinal numeric scale (0-24).\"\"\"\n",
    "    if pd.isna(score):\n",
    "        return np.nan\n",
    "    \n",
    "    s = str(score).strip().upper()\n",
    "    \n",
    "    # Direct mapping for labels and fixed numeric strings\n",
    "    if s in granular_map:\n",
    "        return granular_map[s]\n",
    "    \n",
    "    # Handle simple integer/float scores (e.g., '2' or '1.5') not explicitly in map\n",
    "    try:\n",
    "        num_val = float(s)\n",
    "        # Check if this simple number corresponds to a base ACTFL score\n",
    "        if num_val == 0.9: return granular_map['0.9']\n",
    "        if num_val == 1.0: return granular_map['1.0']\n",
    "        if num_val == 1.5: return granular_map['1.5']\n",
    "        if num_val == 1.9: return granular_map['1.9']\n",
    "        if num_val == 2.0: return granular_map['2.0']\n",
    "        if num_val == 2.5: return granular_map['2.5']\n",
    "        if num_val == 2.9: return granular_map['2.9']\n",
    "        if num_val == 3.0: return granular_map['3.0']\n",
    "    except ValueError:\n",
    "        pass \n",
    "\n",
    "    return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7fff34f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the new standardization function\n",
    "df_marks['Entrance_Ordinal'] = df_marks['Entrance'].apply(standardize_to_ordinal)\n",
    "df_marks['Exit_Ordinal'] = df_marks['Exit'].apply(standardize_to_ordinal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59866ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_anonymized = df_marks.merge(unique_students, on='Student_Key')[['Year', 'Student_ID', 'Entrance_Ordinal', 'Exit_Ordinal']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Standardized Ordinal Score Value Counts (Entrance) ---\")\n",
    "# Only show the top 15 non-NaN values\n",
    "print(df_anonymized['Entrance_Ordinal'].value_counts(dropna=False).sort_index().head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf24b1a",
   "metadata": {},
   "source": [
    "```\n",
    "<class 'pandas.DataFrame'>\n",
    "RangeIndex: 472 entries, 0 to 471\n",
    "Data columns (total 4 columns):\n",
    " #   Column       Non-Null Count  Dtype\n",
    "---  ------       --------------  -----\n",
    " 0   Last Name    472 non-null    str  \n",
    " 1   First Name   472 non-null    str  \n",
    " 2   Student_Key  472 non-null    str  \n",
    " 3   Student_ID   472 non-null    str  \n",
    "dtypes: str(4)\n",
    "memory usage: 14.9 KB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b9ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_anonymized.to_csv(\"upd_preprocessed/upd_merged_anonymized_marks_ordinal_step2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7071207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymization_lookup.to_csv(\"upd_preprocessed/upd_anonymization_lookup_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156eda05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a75372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0f9e2fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_marks_raw = pd.read_csv(\"upd_preprocessed/upd_merged_anonymized_marks_ordinal_step2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9436b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This converts the wide format (one row, two score columns) to a long format (two rows, one score column)\n",
    "df_long = pd.melt(\n",
    "    df_marks_raw,\n",
    "    id_vars=['Year', 'Student_ID'],\n",
    "    value_vars=['Entrance_Ordinal', 'Exit_Ordinal'],\n",
    "    var_name='Test_type_full',\n",
    "    value_name='Mark_Ordinal'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "40cf2d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long['Test_type'] = df_long['Test_type_full'].str.replace('_Ordinal', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e2bab3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Student_ID</th>\n",
       "      <th>Test_type_full</th>\n",
       "      <th>Mark_Ordinal</th>\n",
       "      <th>Test_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>STUDENT_0001</td>\n",
       "      <td>Entrance_Ordinal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Entrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>STUDENT_0002</td>\n",
       "      <td>Entrance_Ordinal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Entrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>STUDENT_0003</td>\n",
       "      <td>Entrance_Ordinal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Entrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>STUDENT_0004</td>\n",
       "      <td>Entrance_Ordinal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Entrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>STUDENT_0005</td>\n",
       "      <td>Entrance_Ordinal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Entrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>2017</td>\n",
       "      <td>STUDENT_0469</td>\n",
       "      <td>Exit_Ordinal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Exit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>2017</td>\n",
       "      <td>STUDENT_0470</td>\n",
       "      <td>Exit_Ordinal</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Exit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>2017</td>\n",
       "      <td>STUDENT_0136</td>\n",
       "      <td>Exit_Ordinal</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Exit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>2017</td>\n",
       "      <td>STUDENT_0471</td>\n",
       "      <td>Exit_Ordinal</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Exit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>2017</td>\n",
       "      <td>STUDENT_0472</td>\n",
       "      <td>Exit_Ordinal</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Exit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>986 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year    Student_ID    Test_type_full  Mark_Ordinal Test_type\n",
       "0    2020  STUDENT_0001  Entrance_Ordinal           NaN  Entrance\n",
       "1    2020  STUDENT_0002  Entrance_Ordinal           NaN  Entrance\n",
       "2    2020  STUDENT_0003  Entrance_Ordinal           NaN  Entrance\n",
       "3    2020  STUDENT_0004  Entrance_Ordinal           NaN  Entrance\n",
       "4    2020  STUDENT_0005  Entrance_Ordinal           NaN  Entrance\n",
       "..    ...           ...               ...           ...       ...\n",
       "981  2017  STUDENT_0469      Exit_Ordinal           NaN      Exit\n",
       "982  2017  STUDENT_0470      Exit_Ordinal           3.0      Exit\n",
       "983  2017  STUDENT_0136      Exit_Ordinal           4.0      Exit\n",
       "984  2017  STUDENT_0471      Exit_Ordinal           5.0      Exit\n",
       "985  2017  STUDENT_0472      Exit_Ordinal           5.0      Exit\n",
       "\n",
       "[986 rows x 5 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4eb205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Dropping rows with missing scores (exams that were not taken/scored)\n",
    "df_final = df_long.dropna(subset=['Mark_Ordinal']).copy()\n",
    "\n",
    "# Selecting final columns and ensure Mark_Ordinal is integer (since it's an ordinal scale)\n",
    "df_final = df_final[['Year', 'Student_ID', 'Test_type', 'Mark_Ordinal']]\n",
    "df_final['Mark_Ordinal'] = df_final['Mark_Ordinal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f136db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Saving the resulting DataFrame\n",
    "output_file = \"upd_preprocessed/upd_marks_for_each_essay_step3.csv\"\n",
    "df_final.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84bb281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae9ec1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "24c16236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a2b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "with open(\"../../data/preprocessed/files.txt\", 'r') as f:\n",
    "    file_paths = [\n",
    "        line.strip().replace('./preprocessed/', '').lstrip('./')\n",
    "        for line in f if line.strip().endswith('.txt')\n",
    "    ]\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "18154523",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup = pd.read_csv(\"upd_preprocessed/upd_anonymization_lookup_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "34b7fde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Student_ID</th>\n",
       "      <th>Test_type</th>\n",
       "      <th>Mark_Ordinal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>STUDENT_0026</td>\n",
       "      <td>Entrance</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>STUDENT_0027</td>\n",
       "      <td>Entrance</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>STUDENT_0028</td>\n",
       "      <td>Entrance</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>STUDENT_0029</td>\n",
       "      <td>Entrance</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>STUDENT_0030</td>\n",
       "      <td>Entrance</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>2017</td>\n",
       "      <td>STUDENT_0468</td>\n",
       "      <td>Exit</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>2017</td>\n",
       "      <td>STUDENT_0470</td>\n",
       "      <td>Exit</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>2017</td>\n",
       "      <td>STUDENT_0136</td>\n",
       "      <td>Exit</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>2017</td>\n",
       "      <td>STUDENT_0471</td>\n",
       "      <td>Exit</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>2017</td>\n",
       "      <td>STUDENT_0472</td>\n",
       "      <td>Exit</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>831 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year    Student_ID Test_type  Mark_Ordinal\n",
       "0    2020  STUDENT_0026  Entrance           4.0\n",
       "1    2020  STUDENT_0027  Entrance           4.0\n",
       "2    2020  STUDENT_0028  Entrance           3.0\n",
       "3    2020  STUDENT_0029  Entrance           3.0\n",
       "4    2020  STUDENT_0030  Entrance           3.0\n",
       "..    ...           ...       ...           ...\n",
       "826  2017  STUDENT_0468      Exit           4.0\n",
       "827  2017  STUDENT_0470      Exit           3.0\n",
       "828  2017  STUDENT_0136      Exit           4.0\n",
       "829  2017  STUDENT_0471      Exit           5.0\n",
       "830  2017  STUDENT_0472      Exit           5.0\n",
       "\n",
       "[831 rows x 4 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores = pd.read_csv(\"upd_preprocessed/upd_marks_for_each_essay_step3.csv\")\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f79e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_essay_path_and_key(path):\n",
    "    \"\"\"\n",
    "    Parses essay file path to extract Student_Key (LASTNAME_FIRSTNAME), \n",
    "    Year, and Test_type ('Entrance' or 'Exit').\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Extracting Year from the path structure\n",
    "    year_match = re.search(r'(2017|2018|2019|2020)', path)\n",
    "    year = int(year_match.group(0)) if year_match else None\n",
    "\n",
    "    # 2. Extracting filename and the name+test_type part\n",
    "    filename = os.path.basename(path)\n",
    "    \n",
    "    # Pattern to capture name parts (e.g., Name_Surname) and the test type (Entry/Exit)\n",
    "    # This assumes the file is ALWAYS First_Name_Last_Name\n",
    "    name_type_match = re.search(r'([A-Za-z]+_[A-Za-z]+).*?(Entry|Exit)\\d', filename, re.IGNORECASE)\n",
    "    \n",
    "    student_key = None\n",
    "    test_type = None\n",
    "\n",
    "    if name_type_match:\n",
    "        # File is First_Name_Last_Name \n",
    "        name_parts = name_type_match.group(1).split('_')\n",
    "        \n",
    "        if len(name_parts) == 2:\n",
    "            first_name = name_parts[0]\n",
    "            last_name = name_parts[1]\n",
    "            \n",
    "            # CRITICAL: Recreate the Student_Key in the standard format: LASTNAME_FIRSTNAME (UPPERCASE)\n",
    "            student_key = f\"{last_name.strip().upper()}_{first_name.strip().upper()}\"\n",
    "        \n",
    "        # Test type extraction (e.g., 'Entry' -> 'Entrance', 'Exit' -> 'Exit')\n",
    "        test_type_raw = name_type_match.group(2)\n",
    "        test_type = 'Entrance' if 'Entr' in test_type_raw else 'Exit' if 'Exit' in test_type_raw else None\n",
    "\n",
    "    return {\n",
    "        'essay_path': path,\n",
    "        'Year': year,\n",
    "        'Test_type': test_type,  # Matches column name in df_scores\n",
    "        'Student_Key': student_key\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0430df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essays_raw = pd.DataFrame([parse_essay_path_and_key(path) for path in file_paths])\n",
    "df_essays_raw = df_essays_raw.dropna(subset=['Student_Key', 'Test_type', 'Year']) \n",
    "\n",
    "\n",
    "df_essays_merged = pd.merge(\n",
    "    df_essays_raw,\n",
    "    df_lookup[['Student_Key', 'Student_ID']].drop_duplicates(), \n",
    "    on='Student_Key',\n",
    "    how='left'\n",
    ")\n",
    "df_essays_merged = df_essays_merged.dropna(subset=['Student_ID'])\n",
    "df_essays_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70191f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(\n",
    "    df_essays_merged,\n",
    "    df_scores,\n",
    "    on=['Student_ID', 'Year', 'Test_type'],\n",
    "    how='inner'\n",
    ")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aba2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f9644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb2e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "82ef20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_essay_content(path):\n",
    "    \"\"\"Reads the content of an essay file, handling common encodings for Russian text.\"\"\"\n",
    "    # Prioritize 'utf-8', but fall back to 'cp1251' or ignore errors if necessary\n",
    "    # 'errors=\"ignore\"' is safest for varied archival data\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            return f.read().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3622f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_essay_path_and_key(path):\n",
    "    \"\"\"\n",
    "    Parses essay file path to extract Student_Key (LASTNAME_FIRSTNAME), \n",
    "    Year, Test_type, and the raw name parts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Extract Year from the path structure\n",
    "    year_match = re.search(r'(2017|2018|2019|2020)', path)\n",
    "    year = int(year_match.group(0)) if year_match else None\n",
    "\n",
    "    # 2. Extract filename and the name+test_type part (e.g., Name_Surname_Entry2)\n",
    "    filename = os.path.basename(path)\n",
    "    \n",
    "    # Pattern to capture name parts (e.g. Name_Surname) and the test type (Entry/Exit)\n",
    "    # The 'Entry'/'Exit' part anchors the identification of the name\n",
    "    name_type_match = re.search(r'([A-Za-z]+_[A-Za-z]+).*?(Entry|Exit)\\d', filename, re.IGNORECASE)\n",
    "    \n",
    "    student_key = None\n",
    "    test_type = None\n",
    "\n",
    "    if name_type_match:\n",
    "        # Filename format is confirmed to be First_Name_Last_Name\n",
    "        name_parts = name_type_match.group(1).split('_')\n",
    "        \n",
    "        if len(name_parts) == 2:\n",
    "            first_name_raw = name_parts[0]\n",
    "            last_name_raw = name_parts[1]\n",
    "            \n",
    "            # CRITICAL: Recreating the Student_Key in the required standard format: LASTNAME_FIRSTNAME (UPPERCASE)\n",
    "            # This ensures matching with the lookup table.\n",
    "            student_key = f\"{last_name_raw.strip().upper()}_{first_name_raw.strip().upper()}\"\n",
    "        \n",
    "        # Test type extraction (e.g., 'Entry' -> 'Entrance', 'Exit' -> 'Exit')\n",
    "        test_type_raw = name_type_match.group(2)\n",
    "        test_type = 'Entrance' if 'Entr' in test_type_raw else 'Exit' if 'Exit' in test_type_raw else None\n",
    "\n",
    "    return {\n",
    "        'essay_path': path,\n",
    "        'Year': year,\n",
    "        'Test_type': test_type,\n",
    "        'Student_Key': student_key\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Loading and Process Data ---\n",
    "\n",
    "# Load all essay file paths\n",
    "with open(\"../../data/preprocessed/files.txt\", 'r') as f:\n",
    "    file_paths = [line.strip().split('./Transcribed ORIGINAL data txt Summer 2017 2018 2019 2020 (1)')[-1] for line in f if line.strip().endswith('.txt')]\n",
    "\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b6b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lookup table (Name -> ID)\n",
    "df_lookup = pd.read_csv(\"upd_preprocessed/upd_anonymization_lookup_table.csv\")\n",
    "df_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bd17ff11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Student_ID</th>\n",
       "      <th>Test_type</th>\n",
       "      <th>Mark_Ordinal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>STUDENT_0026</td>\n",
       "      <td>Entrance</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>STUDENT_0027</td>\n",
       "      <td>Entrance</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>STUDENT_0028</td>\n",
       "      <td>Entrance</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>STUDENT_0029</td>\n",
       "      <td>Entrance</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>STUDENT_0030</td>\n",
       "      <td>Entrance</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>2017</td>\n",
       "      <td>STUDENT_0468</td>\n",
       "      <td>Exit</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>2017</td>\n",
       "      <td>STUDENT_0470</td>\n",
       "      <td>Exit</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>2017</td>\n",
       "      <td>STUDENT_0136</td>\n",
       "      <td>Exit</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>2017</td>\n",
       "      <td>STUDENT_0471</td>\n",
       "      <td>Exit</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>2017</td>\n",
       "      <td>STUDENT_0472</td>\n",
       "      <td>Exit</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>831 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year    Student_ID Test_type  Mark_Ordinal\n",
       "0    2020  STUDENT_0026  Entrance           4.0\n",
       "1    2020  STUDENT_0027  Entrance           4.0\n",
       "2    2020  STUDENT_0028  Entrance           3.0\n",
       "3    2020  STUDENT_0029  Entrance           3.0\n",
       "4    2020  STUDENT_0030  Entrance           3.0\n",
       "..    ...           ...       ...           ...\n",
       "826  2017  STUDENT_0468      Exit           4.0\n",
       "827  2017  STUDENT_0470      Exit           3.0\n",
       "828  2017  STUDENT_0136      Exit           4.0\n",
       "829  2017  STUDENT_0471      Exit           5.0\n",
       "830  2017  STUDENT_0472      Exit           5.0\n",
       "\n",
       "[831 rows x 4 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the prepared scores table (ID, Year, Test_type -> Score)\n",
    "df_scores = pd.read_csv(\"upd_preprocessed/upd_marks_for_each_essay_step3.csv\")\n",
    "df_scores['Mark_Ordinal'] = df_scores['Mark_Ordinal'].astype(float) # Ensure types match for merge\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d138c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Create Essay Metadata DataFrame\n",
    "df_essays_raw = pd.DataFrame([parse_essay_path_and_key(path) for path in file_paths])\n",
    "df_essays_raw = df_essays_raw.dropna(subset=['Student_Key', 'Test_type'])\n",
    "df_essays_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b6854",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essays_with_id = pd.merge(\n",
    "    df_essays_raw,\n",
    "    df_lookup[['Student_Key', 'Student_ID']].drop_duplicates(), \n",
    "    on='Student_Key',\n",
    "    how='left'\n",
    ")\n",
    "df_essays_with_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7e67ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essays_with_id.to_csv('upd_preprocessed/upd_essays_with_id.csv', index=False)\n",
    "df_scores.to_csv('upd_preprocessed/upd_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca57bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(\n",
    "    df_essays_with_id,\n",
    "    df_scores,\n",
    "    on=['Student_ID', 'Year', 'Test_type'],\n",
    "    how='inner' \n",
    ")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1aded08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('upd_preprocessed/upd_training_data_wo_texts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432967c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba280a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c760a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "edfe5e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def read_essay_content(p):\n",
    "    \"\"\"Reads the content of an essay file, handling common encodings for Russian text.\"\"\"\n",
    "    # Prioritize 'utf-8', but fall back to 'cp1251' or ignore errors if necessary\n",
    "    # 'errors=\"ignore\"' is safest for varied archival data\n",
    "    path = '../../data/preprocessed/Transcribed ORIGINAL data txt Summer 2017 2018 2019 2020 (1)/'+p\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            return f.read().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ce258f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_TEXT_COLUMN = 'essay_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f256e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(\"upd_preprocessed/upd_training_data_wo_texts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d8876c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[BERT_TEXT_COLUMN] = df_final['essay_path'].apply(read_essay_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c69e4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('upd_preprocessed/training_data.csv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "95b0be99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Сообщение. Об этом слов мы думаем каждый день....</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Я не знаю.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>У меня не есть словай...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Следуюший, пожалуйста.\"\\nЕсли мне не нравится...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Мне кажется, что личные телефоны и компютери о...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>Привет, ребята! \\nПрограмма почти закончилась....</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>Мой самый интересный день в Миддлбери произоше...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>Мой университет называется Лафайетте. Он наход...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>Честность – важное качество в жизни, но всегда...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>Дорогая г-жа Васильева,\\nЯ изучала русский язы...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1138 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  essay  score\n",
       "0     Сообщение. Об этом слов мы думаем каждый день....    5.0\n",
       "1                                            Я не знаю.    4.0\n",
       "2                              У меня не есть словай...    2.0\n",
       "3     \"Следуюший, пожалуйста.\"\\nЕсли мне не нравится...    4.0\n",
       "4     Мне кажется, что личные телефоны и компютери о...    5.0\n",
       "...                                                 ...    ...\n",
       "1133  Привет, ребята! \\nПрограмма почти закончилась....    7.0\n",
       "1134  Мой самый интересный день в Миддлбери произоше...    7.0\n",
       "1135  Мой университет называется Лафайетте. Он наход...    7.0\n",
       "1136  Честность – важное качество в жизни, но всегда...    7.0\n",
       "1137  Дорогая г-жа Васильева,\\nЯ изучала русский язы...    7.0\n",
       "\n",
       "[1138 rows x 2 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_baseline = pd.DataFrame()\n",
    "df_training_baseline['essay'] = df_final['essay_text']\n",
    "df_training_baseline['score'] = df_final['Mark_Ordinal']\n",
    "df_training_baseline.to_csv('upd_preprocessed/upd_training.csv', index=False)\n",
    "df_training_baseline.to_csv('training_data/upd_training.csv', index=False)\n",
    "df_training_baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-1 (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
